{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "\n",
    "# defining global variable path\n",
    "image_path = \"/home/jesterrexx/Documents/Dataset/CD data/training_set\"\n",
    "\n",
    "# ''function to load folder into arrays and \n",
    "# then it returns that same array'''\n",
    "def loadImages(path):\n",
    "    # Put files into lists and return them as one list of size 4\n",
    "    image_files = sorted([os.path.join(path, 'train', file)\n",
    "         for file in os.listdir(path + \"/train\") if      file.endswith('.jpg')])\n",
    " \n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library Loaded Successfully ..........\n",
      "Reading Dataset from PIckle Object\n",
      "(8005, 80, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import cv2\n",
    "    import os\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    print(\"Library Loaded Successfully ..........\")\n",
    "except:\n",
    "    print(\"Library not Found ! \")\n",
    "\n",
    "\n",
    "class MasterImage(object):\n",
    "\n",
    "    def __init__(self,PATH='', IMAGE_SIZE = 50):\n",
    "        self.PATH = PATH\n",
    "        self.IMAGE_SIZE = IMAGE_SIZE\n",
    "\n",
    "        self.image_data = []\n",
    "        self.x_data = []\n",
    "        self.y_data = []\n",
    "        self.CATEGORIES = []\n",
    "\n",
    "        # This will get List of categories\n",
    "        self.list_categories = []\n",
    "\n",
    "    def get_categories(self):\n",
    "        for path in os.listdir(self.PATH):\n",
    "            if '.DS_Store' in path:\n",
    "                pass\n",
    "            else:\n",
    "                self.list_categories.append(path)\n",
    "        print(\"Found Categories \",self.list_categories,'\\n')\n",
    "        return self.list_categories\n",
    "\n",
    "    def Process_Image(self):\n",
    "        try:\n",
    "            \"\"\"\n",
    "            Return Numpy array of image\n",
    "            :return: X_Data, Y_Data\n",
    "            \"\"\"\n",
    "            self.CATEGORIES = self.get_categories()\n",
    "            for categories in self.CATEGORIES:                                                  # Iterate over categories\n",
    "\n",
    "                train_folder_path = os.path.join(self.PATH, categories)                         # Folder Path\n",
    "                class_index = self.CATEGORIES.index(categories)                                 # this will get index for classification\n",
    "\n",
    "                for img in os.listdir(train_folder_path):                                       # This will iterate in the Folder\n",
    "                    new_path = os.path.join(train_folder_path, img)                             # image Path\n",
    "\n",
    "                    try:        # if any image is corrupted\n",
    "                        image_data_temp = cv2.imread(new_path,cv2.IMREAD_GRAYSCALE)                 # Read Image as numbers\n",
    "                        image_temp_resize = cv2.resize(image_data_temp,(self.IMAGE_SIZE,self.IMAGE_SIZE))\n",
    "                        self.image_data.append([image_temp_resize,class_index])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            data = np.asanyarray(self.image_data)\n",
    "\n",
    "            # Iterate over the Data\n",
    "            for x in data:\n",
    "                self.x_data.append(x[0])        # Get the X_Data\n",
    "                self.y_data.append(x[1])        # get the label\n",
    "\n",
    "            X_Data = np.asarray(self.x_data) / (255.0)      # Normalize Data\n",
    "            Y_Data = np.asarray(self.y_data) \n",
    "\n",
    "            # reshape x_Data\n",
    "\n",
    "            X_Data = X_Data.reshape(-1, self.IMAGE_SIZE, self.IMAGE_SIZE, 1)\n",
    "\n",
    "            return X_Data, Y_Data\n",
    "        except:\n",
    "            print(\"Failed to run Function Process Image \")\n",
    "\n",
    "    def pickle_image(self):\n",
    "\n",
    "        \"\"\"\n",
    "        :return: None Creates a Pickle Object of DataSet\n",
    "        \"\"\"\n",
    "        # Call the Function and Get the Data\n",
    "        X_Data,Y_Data = self.Process_Image()\n",
    "\n",
    "        # Write the Entire Data into a Pickle File\n",
    "        pickle_out = open('X_Data','wb')\n",
    "        pickle.dump(X_Data, pickle_out)\n",
    "        pickle_out.close()\n",
    "\n",
    "        # Write the Y Label Data\n",
    "        pickle_out = open('Y_Data', 'wb')\n",
    "        pickle.dump(Y_Data, pickle_out)\n",
    "        pickle_out.close()\n",
    "\n",
    "        print(\"Pickled Image Successfully \")\n",
    "        return X_Data,Y_Data\n",
    "\n",
    "    def load_dataset(self):\n",
    "\n",
    "        try:\n",
    "            # Read the Data from Pickle Object\n",
    "            X_Temp = open('X_Data','rb')\n",
    "            X_Data = pickle.load(X_Temp)\n",
    "\n",
    "            Y_Temp = open('Y_Data','rb')\n",
    "            Y_Data = pickle.load(Y_Temp)\n",
    "\n",
    "            print('Reading Dataset from PIckle Object')\n",
    "\n",
    "            return X_Data,Y_Data\n",
    "\n",
    "        except:\n",
    "            print('Could not Found Pickle File ')\n",
    "            print('Loading File and Dataset  ..........')\n",
    "\n",
    "            X_Data,Y_Data = self.pickle_image()\n",
    "            return X_Data,Y_Data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/home/jesterrexx/Documents/Dataset/CD data/training_set\"\n",
    "    a = MasterImage(PATH=path,\n",
    "                    IMAGE_SIZE=70)\n",
    "\n",
    "    X_Data,Y_Data = a.load_dataset()\n",
    "    print(X_Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8005, 80, 80, 1)\n",
      "(8005,)\n"
     ]
    }
   ],
   "source": [
    "print(X_Data.shape)\n",
    "print(Y_Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_Data, Y_Data, test_size=0.15, random_state=42)\n",
    "number_of_train = x_train.shape[1]\n",
    "number_of_test = x_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,BatchNormalization,Dropout,MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "    Conv2D(64,(3,3),activation='tanh',padding='same',input_shape=(80,80,1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128,(3,3),activation='tanh',padding='same'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.25),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256,(3,3),activation='tanh',padding='same'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(2,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 80, 80, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 40, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 40, 40, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 20, 20, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 10, 10, 256)       1024      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 51202     \n",
      "=================================================================\n",
      "Total params: 422,658\n",
      "Trainable params: 421,762\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='mae',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "96/96 [==============================] - 121s 1s/step - loss: 0.4133 - acc: 0.4490 - val_loss: 0.4995 - val_acc: 0.4995\n",
      "Epoch 2/15\n",
      "96/96 [==============================] - 117s 1s/step - loss: 0.3858 - acc: 0.4318 - val_loss: 0.4305 - val_acc: 0.4745\n",
      "Epoch 3/15\n",
      "96/96 [==============================] - 114s 1s/step - loss: 0.3788 - acc: 0.4681 - val_loss: 0.4953 - val_acc: 0.4775\n",
      "Epoch 4/15\n",
      "96/96 [==============================] - 114s 1s/step - loss: 0.3581 - acc: 0.4383 - val_loss: 0.4321 - val_acc: 0.4917\n",
      "Epoch 5/15\n",
      "96/96 [==============================] - 118s 1s/step - loss: 0.3585 - acc: 0.4368 - val_loss: 0.4873 - val_acc: 0.5690\n",
      "Epoch 6/15\n",
      "96/96 [==============================] - 119s 1s/step - loss: 0.3426 - acc: 0.4313 - val_loss: 0.4990 - val_acc: 0.5470\n",
      "Epoch 7/15\n",
      "96/96 [==============================] - 116s 1s/step - loss: 0.3524 - acc: 0.4332 - val_loss: 0.3910 - val_acc: 0.4623\n",
      "Epoch 8/15\n",
      "96/96 [==============================] - 114s 1s/step - loss: 0.3392 - acc: 0.4175 - val_loss: 0.4220 - val_acc: 0.4765\n",
      "Epoch 9/15\n",
      "96/96 [==============================] - 125s 1s/step - loss: 0.3382 - acc: 0.4154 - val_loss: 0.4355 - val_acc: 0.5779\n",
      "Epoch 10/15\n",
      "96/96 [==============================] - 120s 1s/step - loss: 0.3324 - acc: 0.4093 - val_loss: 0.4406 - val_acc: 0.4936\n",
      "Epoch 11/15\n",
      "96/96 [==============================] - 116s 1s/step - loss: 0.3028 - acc: 0.3704 - val_loss: 0.3242 - val_acc: 0.3697\n",
      "Epoch 12/15\n",
      "96/96 [==============================] - 117s 1s/step - loss: 0.2839 - acc: 0.4137 - val_loss: 0.3565 - val_acc: 0.4579\n",
      "Epoch 13/15\n",
      "96/96 [==============================] - 115s 1s/step - loss: 0.2867 - acc: 0.3969 - val_loss: 0.4306 - val_acc: 0.5607\n",
      "Epoch 14/15\n",
      "96/96 [==============================] - 116s 1s/step - loss: 0.2815 - acc: 0.4236 - val_loss: 0.3772 - val_acc: 0.4936\n",
      "Epoch 15/15\n",
      "96/96 [==============================] - 116s 1s/step - loss: 0.2849 - acc: 0.4236 - val_loss: 0.3261 - val_acc: 0.4535\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=15,validation_split=0.30,batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f374bfa27fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history_1= model.fit(train_generator,\n\u001b[0m\u001b[1;32m      2\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "history_1= model.fit(train_generator,\n",
    "                       validation_data=test_generator,\n",
    "                              epochs=30,\n",
    "                              steps_per_epoch=125\n",
    "                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2188, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y=to_categorical(y)\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-5d933052de4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,BatchNormalization,Dropout,MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "    Conv2D(64,(3,3),activation='relu',padding='same',input_shape=(100,100,1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128,(3,3),activation='relu',padding='same'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.25),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256,(3,3),activation='relu',padding='same'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(2,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 100, 100, 64)      640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25, 25, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 73730     \n",
      "=================================================================\n",
      "Total params: 445,186\n",
      "Trainable params: 444,290\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='mae',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 108s 1s/step - loss: 0.2714 - acc: 0.4000 - val_loss: 0.4135 - val_acc: 0.4731\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=1,validation_split=0.30,batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
